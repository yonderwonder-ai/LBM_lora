train_shards:
  - "test_dataset/train_single.tar"
validation_shards:
  - "test_dataset/val_single.tar"

# Path to the exact base model created by create_exact_base_model.py
base_model_path: "lora/checkpoints/exact_base.ckpt"

# Path to pre-existing LoRA weights to continue training.
lora_weights_path: "lora/checkpoints/my_lora.safetensors"


# Defines which of the injected LoRA modules should be trained.
# This allows you to freeze some LoRA layers while training others.
# The default is to train all attention layers and all layers in the up_blocks.
trainable_lora_modules:
  - "attn1"
  - "attn2"
  - "up_blocks"

# Training parameters
wandb_project: null
batch_size: 4
learning_rate: 0.0002
optimizer: "AdamW"
save_ckpt_path: "./checkpoints_lora/relighting"
log_interval: 100
max_epochs: 50
save_interval: 1000
num_steps: [1, 2, 4]
